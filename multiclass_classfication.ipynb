{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_0lDkFZq3ohI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.8.0\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.autograph.set_verbosity(0)\n",
    "\n",
    "import keras\n",
    "keras.__version__\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.8.0 in /opt/anaconda3/lib/python3.9/site-packages (2.8.0)\n",
      "Requirement already satisfied: gast>=0.2.1 in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.8.0) (0.5.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.8.0) (3.2.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.8.0) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.8.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.8.0) (1.20.3)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.8.0) (1.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.8.0) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.8.0) (3.10.0.2)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.8.0) (2.8.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.8.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.8.0) (2.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.8.0) (58.0.4)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.8.0) (1.1.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.8.0) (13.0.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.8.0) (3.19.4)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.8.0) (1.1.2)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.8.0) (2.8.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.8.0) (0.24.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.8.0) (1.12.1)\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.8.0) (2.8.0.dev2021122109)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.8.0) (1.44.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow==2.8.0) (0.37.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.0.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.26.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.6.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.3.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (5.0.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/anaconda3/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/anaconda3/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/anaconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.26.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow==2.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bu5jf9gW3ohR"
   },
   "source": [
    "# Classifying newswires: a multi-class classification example\n",
    "\n",
    "In this homework, we will build a network to classify Reuters newswires into 46 different mutually-exclusive topics. Since we have many \n",
    "classes, this problem is an instance of \"multi-class classification\", and since each data point should be classified into only one \n",
    "category, the problem is more specifically an instance of \"single-label, multi-class classification\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-s49gXW3ohT"
   },
   "source": [
    "## The Reuters dataset\n",
    "\n",
    "\n",
    "We will be working with the _Reuters dataset_, a set of short newswires and their topics, published by Reuters in 1986. It's a very simple, \n",
    "widely used toy dataset for text classification. There are 46 different topics; some topics are more represented than others, but each \n",
    "topic has at least 10 examples in the training set.\n",
    "\n",
    "Like IMDB and MNIST, the Reuters dataset comes packaged as part of Keras. Let's take a look right away:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "yjSiBNZS3ohU"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import reuters\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VRrIjNI3ohX"
   },
   "source": [
    "\n",
    "Like with the IMDB dataset, the argument `num_words=10000` restricts the data to the 10,000 most frequently occurring words found in the \n",
    "data.\n",
    "\n",
    "We have 8,982 training examples and 2,246 test examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMdiLzvF3ohf"
   },
   "source": [
    "As with the IMDB reviews, each example is a list of integers (word indices):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "QriVgMSe3ohg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIvaOPin3ohj"
   },
   "source": [
    "Here's how you can decode it back to words, in case you are curious:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "EZg6ME3Y3ohk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The decoded text:? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3\n"
     ]
    }
   ],
   "source": [
    "word_index = reuters.get_word_index()\n",
    "inverted_word_index = dict((i, word) for (word, i) in word_index.items())\n",
    "# Note that our indices were offset by 3\n",
    "# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n",
    "decoded_newswire = ' '.join([inverted_word_index.get(i - 3, '?') for i in train_data[0]])\n",
    "\n",
    "print(\"The decoded text:\" + decoded_newswire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yntI1JS33oho"
   },
   "source": [
    "The label associated with an example is an integer between 0 and 45: a topic index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "swMqUzqz3ohq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  4,  3,  4,  4,  4,  4,  3,  3, 16])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsLn40NK3oht"
   },
   "source": [
    "## Preparing the data\n",
    "\n",
    "We can vectorize the data with the exact same code as in our previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "NDol7z_43ohu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "# Our vectorized training data\n",
    "x_train = vectorize_sequences(train_data)\n",
    "# Our vectorized test data\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9pte7Qr63ohv"
   },
   "source": [
    "\n",
    "Use \"one-hot\" encoding to vectorize the labels. One-hot encoding is a widely used format for categorical data, also called \"categorical encoding\". In our case, one-hot encoding of our labels consists in embedding each label as an all-zero vector with a 1 in the place of the label index. Note that there is a built-in way to do this in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "XO_SmjOD3ohy"
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "one_hot_train_labels = to_categorical(train_labels)\n",
    "one_hot_test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMvTFcc03ohz"
   },
   "source": [
    "## Building our network\n",
    "\n",
    "\n",
    "This topic classification problem looks very similar to our previous movie review classification problem: in both cases, we are trying to \n",
    "classify short snippets of text. There is however a new constraint here: the number of output classes has gone from 2 to 46, i.e. the \n",
    "dimensionality of the output space is much larger. \n",
    "\n",
    "In a stack of `Dense` layers like what we were using, each layer can only access information present in the output of the previous layer. \n",
    "If one layer drops some information relevant to the classification problem, this information can never be recovered by later layers: each \n",
    "layer can potentially become an \"information bottleneck\". In our previous example, we were using 16-dimensional intermediate layers, but a \n",
    "16-dimensional space may be too limited to learn to separate 46 different classes: such small layers may act as information bottlenecks, \n",
    "permanently dropping relevant information.\n",
    "\n",
    "For this reason we will use larger layers. Let's go with 64 units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "HdhwRTjx3ohz"
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-Aj666N3oh1"
   },
   "source": [
    "\n",
    "There are two other things you should note about this architecture:\n",
    "\n",
    "* We are ending the network with a `Dense` layer of size 46. This means that for each input sample, our network will output a \n",
    "46-dimensional vector. Each entry in this vector (each dimension) will encode a different output class.\n",
    "* The last layer uses a `softmax` activation. You have already seen this pattern in the MNIST example. It means that the network will \n",
    "output a _probability distribution_ over the 46 different output classes, i.e. for every input sample, the network will produce a \n",
    "46-dimensional output vector where `output[i]` is the probability that the sample belongs to class `i`. The 46 scores will sum to 1.\n",
    "\n",
    "The best loss function to use in this case is `categorical_crossentropy`. It measures the distance between two probability distributions: \n",
    "in our case, between the probability distribution output by our network, and the true distribution of the labels. By minimizing the \n",
    "distance between these two distributions, we train our network to output something as close as possible to the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "AhmA6zNZ3oh1"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGB_cp8B3oh2"
   },
   "source": [
    "## Validating our approach\n",
    "\n",
    "Let's set apart 1,000 samples in our training data to use as a validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "Y3eyzgOA3oh2"
   },
   "outputs": [],
   "source": [
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIFUZkt43oh3"
   },
   "source": [
    "Now let's train our network for 20 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "MM57jbod3oh3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "16/16 [==============================] - 1s 25ms/step - loss: 2.5840 - accuracy: 0.4977 - val_loss: 1.7491 - val_accuracy: 0.6350\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.4418 - accuracy: 0.7008 - val_loss: 1.3158 - val_accuracy: 0.7160\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.0601 - accuracy: 0.7721 - val_loss: 1.1408 - val_accuracy: 0.7550\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.8341 - accuracy: 0.8218 - val_loss: 1.0330 - val_accuracy: 0.7820\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6664 - accuracy: 0.8593 - val_loss: 0.9672 - val_accuracy: 0.7970\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.5370 - accuracy: 0.8886 - val_loss: 0.9240 - val_accuracy: 0.8110\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.4333 - accuracy: 0.9098 - val_loss: 0.8988 - val_accuracy: 0.8120\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.3536 - accuracy: 0.9255 - val_loss: 0.9125 - val_accuracy: 0.8060\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.2906 - accuracy: 0.9394 - val_loss: 0.9264 - val_accuracy: 0.8110\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.2497 - accuracy: 0.9425 - val_loss: 0.9200 - val_accuracy: 0.8220\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.2153 - accuracy: 0.9476 - val_loss: 0.9195 - val_accuracy: 0.8190\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.1881 - accuracy: 0.9524 - val_loss: 0.9513 - val_accuracy: 0.8150\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.1676 - accuracy: 0.9534 - val_loss: 0.9839 - val_accuracy: 0.8180\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.1537 - accuracy: 0.9541 - val_loss: 0.9757 - val_accuracy: 0.8140\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.1419 - accuracy: 0.9560 - val_loss: 1.0609 - val_accuracy: 0.7930\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.1370 - accuracy: 0.9567 - val_loss: 1.0137 - val_accuracy: 0.8090\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.1244 - accuracy: 0.9572 - val_loss: 1.0594 - val_accuracy: 0.8030\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.1192 - accuracy: 0.9577 - val_loss: 1.0741 - val_accuracy: 0.7980\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.1178 - accuracy: 0.9565 - val_loss: 1.0721 - val_accuracy: 0.8030\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.1177 - accuracy: 0.9565 - val_loss: 1.0471 - val_accuracy: 0.8080\n"
     ]
    }
   ],
   "source": [
    "history= model.fit(partial_x_train,\n",
    "                   partial_y_train,\n",
    "                   epochs=20,\n",
    "                   batch_size=512,\n",
    "                   validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rq5P0gWQ3oh5"
   },
   "source": [
    "Let's display its loss and accuracy curves to help identify when it starts to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "EIRFHE0U3oh5"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TODO: plot the model loss on both training and validation data. \n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Loss on Training and validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "M69rtt143oh6"
   },
   "outputs": [],
   "source": [
    "# TODO: plot prediction accuracy on both training and validation data. \n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Accuracy on Training and validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUarkgZd3oh7"
   },
   "source": [
    "It seems that the network starts overfitting after certain epochs. Let's train a new network from scratch for fewer epochs before it starts overfitting, then let's evaluate it on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "IciZ2dMP3oh8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 1s 22ms/step - loss: 2.6404 - accuracy: 0.5115 - val_loss: 1.7269 - val_accuracy: 0.6550\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.4085 - accuracy: 0.7200 - val_loss: 1.3002 - val_accuracy: 0.7250\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.0440 - accuracy: 0.7781 - val_loss: 1.1350 - val_accuracy: 0.7580\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.8298 - accuracy: 0.8222 - val_loss: 1.0359 - val_accuracy: 0.7890\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6618 - accuracy: 0.8569 - val_loss: 0.9711 - val_accuracy: 0.8050\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.5328 - accuracy: 0.8855 - val_loss: 0.9311 - val_accuracy: 0.8100\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.4279 - accuracy: 0.9103 - val_loss: 0.9125 - val_accuracy: 0.8130\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.3527 - accuracy: 0.9266 - val_loss: 0.9014 - val_accuracy: 0.8150\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.2903 - accuracy: 0.9377 - val_loss: 0.8926 - val_accuracy: 0.8180\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.2485 - accuracy: 0.9437 - val_loss: 0.9030 - val_accuracy: 0.8120\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f904d1cff10>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: retrain the model with the fewer epoches to avoid overfitting\n",
    "# retrain model with changing epoches from 20 to 10\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=10,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 0s 1ms/step - loss: 0.9979 - accuracy: 0.7898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9979169964790344, 0.7898486256599426]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.evaluate(x_test, one_hot_test_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQ3VcXyx3oh_"
   },
   "source": [
    "\n",
    "Your model should reach an accuracy of ~78%. With a balanced binary classification problem, the accuracy reached by a purely random classifier \n",
    "would be 50%, but in our case it is closer to 19%, so our results seem pretty good, at least when compared to a random baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEzjb1m33oiA"
   },
   "source": [
    "## Generating predictions on new data\n",
    "\n",
    "We can verify that the `predict` method of our model instance returns a probability distribution over all 46 topics. Let's generate topic \n",
    "predictions for all of the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "lIuTWvUq3oiB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.1982421e-04, 8.2863960e-05, 9.1070642e-06, ..., 2.8649761e-06,\n",
       "        1.2335543e-05, 1.0714490e-06],\n",
       "       [1.9644583e-02, 4.1045785e-02, 1.0589060e-03, ..., 9.9200185e-04,\n",
       "        1.4567263e-06, 1.2041288e-04],\n",
       "       [2.9863992e-03, 8.4367031e-01, 1.1753496e-02, ..., 2.1306085e-03,\n",
       "        5.8423724e-05, 1.4277019e-03],\n",
       "       ...,\n",
       "       [7.8324105e-05, 7.4441246e-05, 5.9331742e-05, ..., 1.6071022e-05,\n",
       "        6.3637279e-05, 8.8459401e-06],\n",
       "       [1.6084999e-02, 9.9886261e-02, 1.5629636e-02, ..., 8.5532246e-04,\n",
       "        1.5833612e-03, 1.1155083e-03],\n",
       "       [2.4862259e-03, 7.1109128e-01, 3.8301989e-02, ..., 1.1928690e-03,\n",
       "        7.1941846e-05, 1.5264441e-03]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: use the learnt neural network to make a prediction on \n",
    "# the test data \n",
    "predictions = model.predict(x_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1LzmGXx3oiB"
   },
   "source": [
    "Each entry in `predictions` is a vector of length 46. **The** largest entry is the predicted class, i.e. the class with the highest probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "L3RyKAsw3oiB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46,)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: evaluate model performan in terms of accuracy on prediction against the ground truth.\n",
    "predictions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pw3GlSYG3oiH"
   },
   "source": [
    "## On the importance of having sufficiently large intermediate layers\n",
    "\n",
    "\n",
    "We mentioned earlier that since our final outputs were 46-dimensional, we should avoid intermediate layers with much less than 46 hidden \n",
    "units. Now let's try to see what happens when we introduce an information bottleneck by having intermediate layers significantly less than \n",
    "46-dimensional, e.g. 4-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "emfKokRe3oiH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 1s 21ms/step - loss: 3.7301 - accuracy: 0.2791 - val_loss: 3.6288 - val_accuracy: 0.3730\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 3.5219 - accuracy: 0.3829 - val_loss: 3.4451 - val_accuracy: 0.3850\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 3.3134 - accuracy: 0.3918 - val_loss: 3.2500 - val_accuracy: 0.3860\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 3.0945 - accuracy: 0.3974 - val_loss: 3.0380 - val_accuracy: 0.3950\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.8692 - accuracy: 0.4119 - val_loss: 2.8248 - val_accuracy: 0.4100\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.6499 - accuracy: 0.4181 - val_loss: 2.6240 - val_accuracy: 0.4110\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.4495 - accuracy: 0.4217 - val_loss: 2.4407 - val_accuracy: 0.4110\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.2735 - accuracy: 0.4262 - val_loss: 2.2837 - val_accuracy: 0.4180\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 2.1204 - accuracy: 0.4476 - val_loss: 2.1463 - val_accuracy: 0.4430\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.9823 - accuracy: 0.5039 - val_loss: 2.0163 - val_accuracy: 0.5290\n",
      "71/71 [==============================] - 0s 934us/step - loss: 2.0254 - accuracy: 0.5227\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.025362253189087, 0.5227070450782776]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: build a neural network with 4 neuron units in the hidden layer with the validation data \n",
    "# and evaluate its performance on the test data.\n",
    "\n",
    "# NOTE: Keep using retrained model with 10 epochs from now on - for avoiding overfitting\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(4, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(4, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=10,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "results = model.evaluate(x_test, one_hot_test_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M6wV_AR23oiI"
   },
   "source": [
    "\n",
    "You should see the model performance drop. This drop is mostly due to the fact that we are now trying to compress a lot of information (enough information to recover the separation hyperplanes of 46 classes) into an intermediate space that is too low-dimensional. The network is able to cram _most_ of the necessary information into these 8-dimensional representations, but not all of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bFLuR8b3oiI"
   },
   "source": [
    "## Try using larger or smaller hidden layers: 32 units, and 128 units, and see if you will be able to improve the model performance on the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "B1v7pFMGrEik"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 1s 20ms/step - loss: 3.0858 - accuracy: 0.4754 - val_loss: 2.3748 - val_accuracy: 0.5740\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.9638 - accuracy: 0.6396 - val_loss: 1.6954 - val_accuracy: 0.6560\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.4378 - accuracy: 0.7121 - val_loss: 1.4047 - val_accuracy: 0.7030\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.1645 - accuracy: 0.7562 - val_loss: 1.2454 - val_accuracy: 0.7260\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.9814 - accuracy: 0.7920 - val_loss: 1.1503 - val_accuracy: 0.7500\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.8438 - accuracy: 0.8188 - val_loss: 1.0815 - val_accuracy: 0.7740\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.7279 - accuracy: 0.8449 - val_loss: 1.0203 - val_accuracy: 0.7790\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.6323 - accuracy: 0.8642 - val_loss: 0.9783 - val_accuracy: 0.7860\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.5457 - accuracy: 0.8851 - val_loss: 0.9472 - val_accuracy: 0.7960\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4723 - accuracy: 0.9002 - val_loss: 0.9253 - val_accuracy: 0.8040\n",
      "71/71 [==============================] - 0s 902us/step - loss: 1.0179 - accuracy: 0.7756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0178710222244263, 0.7756010890007019]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Try using larger or smaller hidden layers: 32 units, and 128 units\n",
    "\n",
    "# 32 units\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=10,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "results = model.evaluate(x_test, one_hot_test_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 2.1746 - accuracy: 0.5378 - val_loss: 1.4292 - val_accuracy: 0.6850\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 1.1105 - accuracy: 0.7641 - val_loss: 1.1063 - val_accuracy: 0.7530\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.7616 - accuracy: 0.8423 - val_loss: 0.9844 - val_accuracy: 0.7920\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.5646 - accuracy: 0.8835 - val_loss: 0.8985 - val_accuracy: 0.8180\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.4105 - accuracy: 0.9107 - val_loss: 0.9289 - val_accuracy: 0.8140\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.3105 - accuracy: 0.9334 - val_loss: 0.8921 - val_accuracy: 0.8200\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.2493 - accuracy: 0.9436 - val_loss: 0.9288 - val_accuracy: 0.8110\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 0.2111 - accuracy: 0.9485 - val_loss: 0.9152 - val_accuracy: 0.8080\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 0.1764 - accuracy: 0.9541 - val_loss: 0.9087 - val_accuracy: 0.8180\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 22ms/step - loss: 0.1690 - accuracy: 0.9510 - val_loss: 0.9623 - val_accuracy: 0.8100\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 1.0577 - accuracy: 0.7872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0577466487884521, 0.7871772050857544]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 128 units\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(128, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=10,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "results = model.evaluate(x_test, one_hot_test_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The performance on the test data has no big improvement, with 32 or 128 units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJ1JPiTErD7X"
   },
   "source": [
    "\n",
    "## We were using two hidden layers. Now try to use a single hidden layer, or three hidden layers, and see if you will be able to improve the model performance on the test data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "6zukIClyrh3k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 1s 23ms/step - loss: 2.5219 - accuracy: 0.5506 - val_loss: 1.7625 - val_accuracy: 0.6580\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.4477 - accuracy: 0.7239 - val_loss: 1.3269 - val_accuracy: 0.7210\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.0635 - accuracy: 0.7840 - val_loss: 1.1380 - val_accuracy: 0.7600\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.8308 - accuracy: 0.8311 - val_loss: 1.0202 - val_accuracy: 0.7760\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6648 - accuracy: 0.8671 - val_loss: 0.9336 - val_accuracy: 0.8050\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.5410 - accuracy: 0.8946 - val_loss: 0.8915 - val_accuracy: 0.8110\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.4442 - accuracy: 0.9124 - val_loss: 0.8681 - val_accuracy: 0.8140\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.3698 - accuracy: 0.9246 - val_loss: 0.8343 - val_accuracy: 0.8160\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.3097 - accuracy: 0.9361 - val_loss: 0.8119 - val_accuracy: 0.8240\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.2627 - accuracy: 0.9424 - val_loss: 0.8245 - val_accuracy: 0.8260\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.9096 - accuracy: 0.7956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9096263647079468, 0.7956367135047913]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Try to use a single hidden layer, or three hidden layers\n",
    "\n",
    "# Try single layer, 64 units\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=10,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "results = model.evaluate(x_test, one_hot_test_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The perfromance on test data has been better by using single hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 1s 25ms/step - loss: 2.6863 - accuracy: 0.4937 - val_loss: 1.7433 - val_accuracy: 0.5990\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.4529 - accuracy: 0.6904 - val_loss: 1.3325 - val_accuracy: 0.7130\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 1.0840 - accuracy: 0.7611 - val_loss: 1.1587 - val_accuracy: 0.7440\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.8599 - accuracy: 0.8158 - val_loss: 1.0645 - val_accuracy: 0.7780\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.6884 - accuracy: 0.8530 - val_loss: 1.0115 - val_accuracy: 0.7910\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.5552 - accuracy: 0.8791 - val_loss: 1.0114 - val_accuracy: 0.7880\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.4495 - accuracy: 0.9028 - val_loss: 0.9602 - val_accuracy: 0.8040\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.3617 - accuracy: 0.9208 - val_loss: 0.9926 - val_accuracy: 0.8070\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.3016 - accuracy: 0.9334 - val_loss: 1.0711 - val_accuracy: 0.7920\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.2557 - accuracy: 0.9407 - val_loss: 1.0246 - val_accuracy: 0.8090\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 1.1136 - accuracy: 0.7818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1136127710342407, 0.7818343639373779]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=10,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "results = model.evaluate(x_test, one_hot_test_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The perfromance on test data has been slightly worse by using three hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gldwCoKB3oiI"
   },
   "source": [
    "## Wrapping up\n",
    "\n",
    "\n",
    "Here's what you should take away from this example:\n",
    "\n",
    "* If you are trying to classify data points between N classes, your network should end with a `Dense` layer of size N.\n",
    "* In a single-label, multi-class classification problem, your network should end with a `softmax` activation, so that it will output a \n",
    "probability distribution over the N output classes.\n",
    "* _Categorical crossentropy_ is almost always the loss function you should use for such problems. It minimizes the distance between the \n",
    "probability distributions output by the network, and the true distribution of the targets.\n",
    "* There are two ways to handle labels in multi-class classification:\n",
    "    ** Encoding the labels via \"categorical encoding\" (also known as \"one-hot encoding\") and using `categorical_crossentropy` as your loss \n",
    "function.\n",
    "    ** Encoding the labels as integers and using the `sparse_categorical_crossentropy` loss function.\n",
    "* If you need to classify data into a large number of categories, then you should avoid creating information bottlenecks in your network by having \n",
    "intermediate layers that are too small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKekl-HuGXiU"
   },
   "source": [
    "## Bonus Point\n",
    "Can you think of other methods to further improve the model performance? Code it up and evaluate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "38I30otcrO-r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_33 (Dense)            (None, 64)                640064    \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 46)                2990      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 651,374\n",
      "Trainable params: 651,374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 1s 24ms/step - loss: 2.7842 - accuracy: 0.5073 - val_loss: 1.8881 - val_accuracy: 0.6340\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.6087 - accuracy: 0.7038 - val_loss: 1.4969 - val_accuracy: 0.7190\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.2756 - accuracy: 0.7635 - val_loss: 1.3453 - val_accuracy: 0.7460\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.0782 - accuracy: 0.8084 - val_loss: 1.2679 - val_accuracy: 0.7670\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.9458 - accuracy: 0.8381 - val_loss: 1.1894 - val_accuracy: 0.7940\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.8488 - accuracy: 0.8612 - val_loss: 1.1567 - val_accuracy: 0.7830\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.7678 - accuracy: 0.8814 - val_loss: 1.1214 - val_accuracy: 0.8170\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.7049 - accuracy: 0.8964 - val_loss: 1.1298 - val_accuracy: 0.8000\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.6589 - accuracy: 0.9100 - val_loss: 1.0979 - val_accuracy: 0.8190\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.6168 - accuracy: 0.9179 - val_loss: 1.1069 - val_accuracy: 0.8040\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 1.2109 - accuracy: 0.7783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2109249830245972, 0.7782725095748901]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Try L2 weight regularization to improve the performance.\n",
    "# Alpha = 0.001\n",
    "from keras import regularizers\n",
    "\n",
    "model_L2 = models.Sequential()\n",
    "model_L2.add(layers.Dense(64, kernel_regularizer=regularizers.l2(0.001), activation='relu', input_shape=(10000,)))\n",
    "model_L2.add(layers.Dense(64, kernel_regularizer=regularizers.l2(0.001), activation='relu'))\n",
    "model_L2.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model_L2.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_L2.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=10,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "results = model_L2.evaluate(x_test, one_hot_test_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 1s 23ms/step - loss: 2.5694 - accuracy: 0.5457 - val_loss: 1.8154 - val_accuracy: 0.6530\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.5223 - accuracy: 0.7212 - val_loss: 1.4183 - val_accuracy: 0.7180\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.1887 - accuracy: 0.7844 - val_loss: 1.2467 - val_accuracy: 0.7640\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.0028 - accuracy: 0.8215 - val_loss: 1.1513 - val_accuracy: 0.7890\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.8819 - accuracy: 0.8467 - val_loss: 1.1018 - val_accuracy: 0.7980\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.7925 - accuracy: 0.8713 - val_loss: 1.0603 - val_accuracy: 0.8070\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.7263 - accuracy: 0.8880 - val_loss: 1.0353 - val_accuracy: 0.8090\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.6746 - accuracy: 0.8991 - val_loss: 1.0042 - val_accuracy: 0.8200\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.6320 - accuracy: 0.9090 - val_loss: 0.9860 - val_accuracy: 0.8180\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.5955 - accuracy: 0.9184 - val_loss: 0.9841 - val_accuracy: 0.8150\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 1.0610 - accuracy: 0.7979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0609667301177979, 0.7978628873825073]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_L2 = models.Sequential()\n",
    "model_L2.add(layers.Dense(64, kernel_regularizer=regularizers.l2(0.001), activation='relu', input_shape=(10000,)))\n",
    "model_L2.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model_L2.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_L2.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=10,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "results = model_L2.evaluate(x_test, one_hot_test_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using single layer, 64 units in L2 model shows the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 1s 27ms/step - loss: 2.6434 - accuracy: 0.5232 - val_loss: 1.7218 - val_accuracy: 0.6310\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.4715 - accuracy: 0.6863 - val_loss: 1.3069 - val_accuracy: 0.7140\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 1.1308 - accuracy: 0.7509 - val_loss: 1.1483 - val_accuracy: 0.7560\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.9282 - accuracy: 0.7881 - val_loss: 1.0617 - val_accuracy: 0.7640\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.7817 - accuracy: 0.8230 - val_loss: 0.9856 - val_accuracy: 0.7880\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.6503 - accuracy: 0.8519 - val_loss: 0.9602 - val_accuracy: 0.7910\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.5595 - accuracy: 0.8747 - val_loss: 0.9190 - val_accuracy: 0.8090\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.4660 - accuracy: 0.8994 - val_loss: 0.9065 - val_accuracy: 0.8120\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 17ms/step - loss: 0.3892 - accuracy: 0.9128 - val_loss: 0.8844 - val_accuracy: 0.8130\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.3413 - accuracy: 0.9246 - val_loss: 0.8804 - val_accuracy: 0.8210\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.9659 - accuracy: 0.7939\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9658802151679993, 0.7938557267189026]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Try drop-out model\n",
    "# p = 0.1\n",
    "model_drop = models.Sequential()\n",
    "model_drop.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model_drop.add(layers.Dropout(0.1))\n",
    "model_drop.add(layers.Dense(64, activation='relu'))\n",
    "model_drop.add(layers.Dropout(0.1))\n",
    "model_drop.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model_drop.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_drop.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=10,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "results = model_drop.evaluate(x_test, one_hot_test_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 1s 23ms/step - loss: 2.5730 - accuracy: 0.5563 - val_loss: 1.7897 - val_accuracy: 0.6550\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 1.4755 - accuracy: 0.7152 - val_loss: 1.3414 - val_accuracy: 0.7190\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.0942 - accuracy: 0.7769 - val_loss: 1.1369 - val_accuracy: 0.7560\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.8691 - accuracy: 0.8193 - val_loss: 1.0255 - val_accuracy: 0.7770\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.7128 - accuracy: 0.8563 - val_loss: 0.9543 - val_accuracy: 0.7990\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.5924 - accuracy: 0.8795 - val_loss: 0.9038 - val_accuracy: 0.8140\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.4961 - accuracy: 0.9008 - val_loss: 0.8686 - val_accuracy: 0.8150\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.4236 - accuracy: 0.9126 - val_loss: 0.8519 - val_accuracy: 0.8180\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.3577 - accuracy: 0.9262 - val_loss: 0.8316 - val_accuracy: 0.8200\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 16ms/step - loss: 0.3078 - accuracy: 0.9347 - val_loss: 0.8186 - val_accuracy: 0.8290\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.9008 - accuracy: 0.7979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9007887840270996, 0.7978628873825073]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# single layer\n",
    "model_drop = models.Sequential()\n",
    "model_drop.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model_drop.add(layers.Dropout(0.1))\n",
    "model_drop.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model_drop.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_drop.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=10,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "results = model_drop.evaluate(x_test, one_hot_test_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The performance has slightly improved by using drop-out model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7CGM-0QrOrp"
   },
   "source": [
    "# End of HW1"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MSIS549_HW1_newswires.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
